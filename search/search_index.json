{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Hi. I'm Abhiram <p>     I'm a Machine Learning Engineer at O9 Solutions, a supply chain based company in Bangalore. I like coding in Python and R, reading fiction/non-fiction and writing short stories. I co-created and run Broke Bibliophiles Bangalore, one of the largest bookclubs in Bangalore (2017 - present). <p>  If you'd like to learn new concepts in and using Python and Data/ML Engineering, subscribe to my newsletter and you'll be notified with each new issue - https://everythingpython.substack.com!     I also have a Youtube channel for people who'd like to learn by a see-and-learn method, Subscribe here - EverythingPython! </p> <p> If you'd like to talk about any of the above or just tell me about something in Greek Mythology, mail me.</p>"},{"location":"blog/","title":"Hello","text":""},{"location":"blog/2019/02/22/bangpypers-preworkshop-setup/","title":"Bangpypers PreWorkshop Setup","text":"<p>A guide to installing a newer version of Python is available here.</p> <p>We at Bangpypers conduct a lot of workshops and some steps are common for all of them. To that end, this post is meant to serve as guide for people to install Python on Ubuntu and Windows (if required), setting up virtualenv and installing the package(s) germane to the corresponding Workshop.</p>"},{"location":"blog/2019/02/22/bangpypers-preworkshop-setup/#ubuntu","title":"Ubuntu","text":"<ul> <li>Check your existing version of Python to see if you already have what's required.</li> </ul> <pre><code> $python --version\n</code></pre> <ul> <li>Open terminal using <code>Ctrl+Alt+T</code> or searching for \u201cTerminal\u201d from app launcher. Once it opens, run the following command to add the  Personal Package Archive (PPA):</li> </ul> <pre><code> $sudo add-apt-repository ppa:jonathonf/python-3.6\n</code></pre> <ul> <li>Update packages and install Python 3.6 using the following commands:</li> </ul> <pre><code> $sudo apt-get update\n $sudo apt-get install python3.6\n $sudo apt-get install python3-pip\n $sudo apt-get install python3.6-dev\n</code></pre> <ul> <li>Install virtualenv:</li> </ul> <pre><code> $sudo apt-get install virtualenv\n</code></pre> <ul> <li>Create a new virtual environment and activate it:</li> </ul> <pre><code> $cd ~\n $mkdir twisted_workshop\n $cd twisted_wokshop\n $virtualenv -p python3.6 venv\n $source venv/bin/activate\n $pip install twisted (For the Twisted Workshop)\n</code></pre>"},{"location":"blog/2019/02/22/bangpypers-preworkshop-setup/#windows","title":"Windows","text":"<ul> <li>Download and install the binary from https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe</li> </ul> <ul> <li>Add the location of the Python folder at the point of installation to the $PATH environment variable.</li> </ul> <ul> <li>Save the file from here https://bootstrap.pypa.io/get-pip.py and run it using :</li> </ul> <pre><code> C:\\Users\\abhiram\\Desktop&gt;python get-pip.py\n</code></pre> <ul> <li>Install virtualenv :</li> </ul> <pre><code> C:\\Users\\abhiram\\Desktop&gt;pip install virtualenv\n</code></pre> <ul> <li>Create a new virtual environment and install Twisted in it :</li> </ul> <pre><code> C:\\Users\\abhiram\\Desktop&gt;virtualenv -p python3.6 venv\n C:\\Users\\abhiram\\Desktop&gt;venv/Scripts/activate\n C:\\Users\\abhiram\\Desktop&gt;pip install twisted (For the Twisted Workshop)\n</code></pre> <p>I hope this helps in setting up the environment so we can get minimize time required in getting started with the meat of the workshop :) If you have any questions, leave them in the comments and I'll get back to you.</p> <p>For details related to Bangpypers, check the website and the meetup page.</p> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2019/12/10/custom-css-jupyterlab-ext/","title":"Custom CSS Jupyterlab Ext","text":"<p>Jupyterlab is a definite improvement on the older IPython notebook interface - both in features and in appearance. There is now even an in-built \"Dark Theme\" that can be enabled. </p> <p>But is that all?</p> <p>As we know, Jupyterlab is a browser based app and is ipso facto, written on a base of HTML, CSS and Javascript. So if we want to change the appearance over and above what we get out of the box with jupyterlab, we can.  Now, it is possible to make any CSS changes by hacking into the internals of the notebook, but thanks to a nifty Jupyterlab extension by Adam Wallner, we don't have to.</p> <p>Check out Step 4 of my previous post for preliminary installation steps, if you don't already have Jupyterlab installed.</p>"},{"location":"blog/2019/12/10/custom-css-jupyterlab-ext/#steps-","title":"Steps -","text":"<p>a) Install custom_css</p> <pre><code> jupyter labextension install @wallneradam/custom_css\n</code></pre> <p>b) Run jupyter-lab</p> <pre><code> jupyter-lab\n</code></pre> <p>In Jupyter-lab</p> <p>c) Enable the Extensions Manager (experimental)</p> <p></p> <p>d) Navigate to the advanced settings editor</p> <p></p> <p>e) Navigate to the custom-css pane and your \"User Preferences\" section is expected to be largely empty unlike in the populated figure below.</p> <p></p> <p>f) Let me explain the block of code added above in \"User Preferences\" (sans comments)</p> <pre><code>{\n  \"rules\": \n  [\n    {\n    \"selector\": \"#jp-MainLogo\",\n    \"styles\": \n    [\n    \"display:block\",\n    \"background: url('https://abhiramr.com/img/sharingan.jpeg') no-repeat\",\n    \"background-size: contain\",\n    \"width: 23px\",\n    \"height: 45px\",\n    \"padding-left: 0px\",\n    \"-moz-box-sizing: border-box\",\n    \"box-sizing: border-box\"\n    ]\n    },\n    {\n    \"selector\": \".jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser\",\n    \"styles\": [\"background:#50bd30\"]\n    }\n  ]\n}\n</code></pre> <p>We have a list of <code>rules</code> whose members are elements that are to be styled. Each element member is a dictionary, identified by a <code>selector</code> that can be a tag, a class or an id (or any selector). Each member also consists of list - <code>style</code> in which we list out every custom property we wish to bestow upon the element. If a particular element is identified by multiple ids or classes as is normally the case, they are just all listed in the <code>selector</code> separated by spaces.</p> <p>In this example, I've chosen to modify two elements - (i) the icon on the top left corner of the notebook that is normally the standard Jupyter notebook icon and (ii) The highlighted color that indicates the current active cell in the notebook. </p> <p>And now, in classic weight-loss program fashion, here are the before and after pics for each of the changes made.</p>"},{"location":"blog/2019/12/10/custom-css-jupyterlab-ext/#before","title":"Before :","text":""},{"location":"blog/2019/12/10/custom-css-jupyterlab-ext/#after","title":"After :","text":""},{"location":"blog/2019/12/10/custom-css-jupyterlab-ext/#finally-","title":"Finally -","text":"<p>This is barely the tip of the iceberg. The number of tweaks you can make to your Notebook are limited only by your HTML and CSS savvy!</p>"},{"location":"blog/2019/12/10/custom-css-jupyterlab-ext/#additional-reference-links-","title":"Additional reference links -","text":"<ul> <li>https://github.com/wallneradam/jupyterlab-custom-css</li> </ul> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/","title":"Data Visualization Exp1","text":""},{"location":"blog/2017/09/27/data-visualization-exp1/#hours-of-max-fb-usage","title":"Hours of max FB Usage \u2014","text":"<p>I originally intended to do something else, but ended up doing this instead\u200a \u2014\u200a Figuring out at what hours during the day I\u2019ve been most active on Facebook\u200a\u2014\u200aoutliers included.</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#how","title":"How?","text":"<ul> <li> <p>Downloaded my entire Facebook data - This can be done using the Facebook APIs or using the less painful way\u200a\u2014\u200arequesting for a copy of your Facebook data as a zip file from your Settings panel.</p> </li> <li> <p>Scraped this local webpage\u2019s timeline section after hosting it on a local server with a simple Python script using Scrapy and extracted only the timestamps for all the posts and stored it in a CSV.</p> </li> <li> <p>Parsed this data (a modest dataset of 8500+ entries) and fetched the timestamps taking heed of AM and PM and storing this data in another CSV.</p> </li> <li> <p>Segregated this data into hourly ranges e.g. 1am-2am, 2am-3am , for all 24 hours and obtain the cumulative count of entries per hour and store this in another final CSV. Surprisingly, MS-Excel was very helpful in this venture.</p> </li> <li> <p>The CSV from Step 4 served as input to the D3.JS bubble chart script (Courtesy Mike Bostock\u2019s bubble chart template, with some tweaks).</p> </li> </ul> <p>The result was this gloriously satisfying and insightful bubble chart.</p> <p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#what-does-this-show","title":"What does this show?","text":"<p>The colors have no bearing, but the sizes of the bubbles indicate the number of posts, well, posted in that hour\u200a\u2014\u200athe larger bubbles indicating more activity during that period and the smallest ones being the least amount of activity in that hour. The inference being that I have wasted a tremendous amount of time on Facebook during almost every hour of the day over the last 9 year time-period for which this data was obtained, save for the 2am\u200a\u2014\u200a6am mark. This also illustrates a seemingly poor sleep pattern.</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#what-havent-i-taken-into-account","title":"What haven\u2019t I taken into account?","text":"<p>A lot of people post on my wall on Facebook on my birthday. But this number which varies around the 100\u2013150 mark every year is scattered across all hours (around 10 posts approx. every hour) of the day and this outlier, I feel, can be safely neglected.</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#whats-next","title":"What\u2019s next?","text":"<p>I\u2019m hoping to glean some more insights (useful ones hopefully) from this data and will post the results as a follow-up to this blog.</p> <p>Feel free to try this out for yourself if you\u2019re interested in insights that you can probably intuitively already find out without going through this procedure or are just looking to wade into the basics of Data Visualization just to feel like you\u2019ve started something rudimentary along these lines for your satisfaction ..like I did.</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#reference-material","title":"Reference material \u2014","text":"<p>https://www.facebook.com/help/302796099745838 https://doc.scrapy.org/en/latest/topics/shell.html https://bl.ocks.org/mbostock/4063269</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#source-code","title":"Source Code \u2014","text":"<p>https://github.com/abhiramr/DataViz/tree/master/Fb_Usage</p>"},{"location":"blog/2017/09/27/data-visualization-exp1/#also-published-in","title":"Also Published In \u2014","text":"<p>Hacker Noon</p> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2017/11/27/data-visualization-exp2/","title":"Data Visualization Exp2","text":"<p>Alright. Round 2 . FIGHT!</p> <p>So, the last time, I figured out, unintentionally, at what times of the day most of my posts happened. This time, I again unintentionally got results different from what I set out to do\u200a\u2014\u200aI\u2019ll get to this eventually I guess, but for now, the objective is obtaining a measure of the number of posts I make per day and thereby see if I can figure out which days of the week have been the most (least?) productive. Of course, this is still a step away from my eventual goal (hopefully Experiment 3 should sort it out). Let\u2019s get to the methodology \u2014</p>"},{"location":"blog/2017/11/27/data-visualization-exp2/#step-1-how-do-i-get-the-data","title":"Step 1\u200a\u2014\u200aHow do I get the data?","text":"<p>Remember how I downloaded my entire data last time and web-scraped it using scrapy? Well, this time I API-ed my way into getting my data because for a higher level of specificity, sometimes just HTML scraping won\u2019t do. Timestamps were easy enough to fetch from the earlier case, but here I needed date values that were hard to get from the Downloaded Data.zip file. So this data would comprise of the entire stream of my posts all the way from 2008\u200a\u2014\u200awhen I joined FB to the 23rd of Nov, upto when I retrieved my post details.</p> <ol> <li> <p>Get your user access token\u200a\u2014\u200a I\u2019m sure there\u2019s an OAuth API way to do this, but I did this by manually generating an Access token from the Graph API explorer or Access Token Tool. The former lets you choose permissions on the access token as to what APIs you can open the token up to and also lets you see the expiry time for that token (it\u2019s around an hour from the time of creation. This is the annoying part. Security aside, I\u2019d have really loved a longer expiry time for the access token, but I\u2019m sure once I figure out the API to generate the access token, I can just have my script\u200a\u2014\u200asee below\u200a\u2014\u200afetch it automatically everytime it hits a 401)</p> </li> <li> <p>Get the APIs necessary\u200a\u2014\u200a Given the nature of my pivot, just the feed API would have done the job for me. But I\u2019ll be using the result of the reactions API in my next blog post, so that\u2019s what I ended up using here as well, in conjunction with the feed API.</p> </li> </ol> <p>A typical feed API call looks like this\u200a\u2014\u200a</p> <pre><code>https://graph.facebook.com/v2.5/10152460834423273/feed?limit=25&amp;access_token=EAACEdEose0cBANI6UZBB77zEUEOpKNhxyZA\\\nuLJozw2uQ18bNIW18Qp1zxSUh6OZC0gONp3B5ZCZCZABqdqWxkg9rcvdleay0BVciZCZA46TMk\\\nYog60RZBdnwfpZAaVVMhMKgN90RLMAeJWoKk1eByd\\\nwXZAU16oZCmoCF1BpCLKK3e4kUGudMHMV11ZBPHZAobYwFUgybgZD&amp;until=1500849601 \n</code></pre> <ul> <li> <p>The version number matters because there are some APIs that version-dependant. For example, the reactions API requires v2.6 or higher.</p> </li> <li> <p>The second URI parameter is the user ID that you can also get from the Graph API explorer.</p> </li> <li> <p>The third parameter\u200a\u2014\u200alimit\u200a\u2014\u200ais optional because the default value is 25. You can include it if you want to specify higher values i.e fetching more results per page.</p> </li> <li> <p>The fourth parameter is the access token itself obtained in Step (a)</p> </li> <li> <p>The fifth parameter\u200a\u2014\u200a\u2018until\u2019\u200a\u2014\u200ais a misnomer. It actually indicates the starting datetime object from when you want to fetch the #limit posts.</p> </li> </ul> <p>The response of this API is of the form :</p> <pre><code>{\n\"data\" : [\n{\"message\": \"FB_Post body_1\"\n \"created_time\": \"2017\u201311\u201327T12:47:50+0000\"\n \"id\": \"1012460834423273_10156427233338273\"},\n{\"message\": \"FB_Post body_2\"\n \"created_time\": \"2017\u201311\u201327T10:47:50+0000\"\n \"id\": \"1012460834423273_10156427233338273\"}\n.\n.\n25 entries ],\n\"paging\": { \n\"previous\": \"long obnoxious link1\",\n\"next\" : \"long obnoxious link2\"}\n}\n</code></pre> <p>I figured that the seemingly useful \"next\u201d link is rendered unnecessary if you can find out the date of the last post on that page yourself, convert it into Epoch time and use that value in the \"until\u201d parameter mentioned above.</p> <p>Also, in the response is an inconspicuous little \"id\u201d. This little monster is actually an amalgamation of your user_id and the post\u2019s id itself, resulting in another unique id. This is the id to use in the Reactions API that results in obtaining all the reactions for that post enumerated as one of-</p> <pre><code>  {\"LIKE\u201d, \"ANGRY\u201d, \"WOW\u201d, \"HAHA\u201d, \"LOVE\u201d}\n</code></pre> <p>And a typical Reactions API looks like this \u2014</p> <pre><code>https://graph.facebook.com/v2.6/10152460834423273_10156395770753273/reactions?limit=25&amp;access_token=EAACEdEose0cBANI6UZBB77zEUEOpKNhxyZAuLJozw2uQ18bNIW18Qp1zxSUh6OZC0gONp3B5ZCZCZABqdqWxkg9rcvdleay0BVciZCZA46TMkYog60RZBdnwfpZAaVVMhMKgN90RLMAeJWoKk1eBydwXZAU16oZCmoCF1BpCLKK3e4kUGudMHMV11ZBPHZAobYwFUgybgZD\n</code></pre> <p>And the response is \u2014</p> <pre><code>    {\n     \"data\": [\n     {\n     \"id\": \"1022218239209614\",\n     \"name\": \"Name1\",\n     \"type\": \"HAHA\"\n     },\n     {\n     \"id\": \"86372147016752\",\n     \"name\": \"Name2\",\n     \"type\": \"WOW\"\n     },\n     .\n     .\n\n    ],\n     \"paging\": {\n     \"cursors\": {\n     \"before\": \"TVRVNU9EZAzBNamN5TlRveE5URXhNakEyTVRFMU9qYzRPRFkwT0RBek56a3hNek14TWc9PQZDZD\",\n     \"after\": \"TlRjMk1EWXpNVFF6T2pFMU1URXdNekE0TmpnNk1qVTBNRGsyTVRZAeE13PT0ZD\"\n     },\n     \"next\": \"https://graph.facebook.com/v2.6/10152460834423273_10156395770753273/reactions?access_token=EAACEdEose0cBANI6UZBB77zEUEOpKNhxyZAuLJozw2uQ18bNIW18Qp1zxSUh6OZC0gONp3B5ZCZCZABqdqWxkg9rcvdleay0BVciZCZA46TMkYog60RZBdnwfpZAaVVMhMKgN90RLMAeJWoKk1eBydwXZAU16oZCmoCF1BpCLKK3e4kUGudMHMV11ZBPHZAobYwFUgybgZD&amp;pretty=1&amp;limit=25&amp;after=TlRjMk1EWXpNVFF6T2pFMU1URXdNekE0TmpnNk1qVTBNRGsyTVRZAeE13PT0ZD\"\n     }\n    }\n</code></pre> <p>Remember the good ol\u2019 days when there was just a Like button? That\u2019s gone now, so even the simple Like API returns only the number of \u201cLike\u201d reactions you got on a post. So if you wanted the number of Total reactions on your post, the above mentioned API is what you\u2019d want.</p> <p>Anyway, like I mentioned earlier, to track the number of posts, I could have made do with the Feed API, but using the Feeds, followed by the Reactions API and painstakingly making API calls limiting myself to 25 each time (I didn\u2019t want to get blocked by FB for making too many API calls\u200a\u2014\u200awhich in hindsight was foolish because every click on the website is an API call) , obtaining the last date per page and substituting that in the next API call, I ended up making a CSV of {Total Reactions per post, Date} netting upto around 4500 entries. Next I computed the number of posts per date and obtained another CSV out of that\u200a\u2014\u200a{#of posts per date, Date}.</p> <p>I used Python for the entire process \u2014Requests, Numpy and CSV. I used matplotlib for some test graphs as well. The IPython notebooks for the same are provided down below.</p> <p>Now the data is ready.</p>"},{"location":"blog/2017/11/27/data-visualization-exp2/#step-2-plotting-all-of-this-in-a-nice-format","title":"Step 2\u200a\u2014\u200aPlotting all of this in a nice format","text":"<p>Last time, I\u2019d used a bubble chart to highlight the time of maximum usage. This time, given that it\u2019s panning over a long period of time, I decided to use a Calendar Heatmap rendered again, using D3.JS . Now I\u2019m aware that there are A LOT of different ways to visualise this data. But this is what I went with. I\u2019ll play around with others soon :)</p> <p>For this, I used a variation of [1], extending the time period to start from Q1 of 2008 as opposed to last year as in [1]. Also, I modified some more parameters to suit my use case, in terms of the width of panning, color ranges for the map, caption etc.</p> <p>The result was this heatmap\u200a\u2014\u200aa sample of it is presented below.</p> <p></p>"},{"location":"blog/2017/11/27/data-visualization-exp2/#step-3-pretend-this-hasnt-been-an-enormous-waste-of-time-and-try-to-justify-what-all-of-this-potentially-tells-me","title":"Step 3 : Pretend this hasn\u2019t been an enormous waste of time and try to justify what all of this potentially tells me \u2014","text":"<p>It\u2019s still very much an intermediate step into my eventual goal. But what this tells me is what days of the week I\u2019ve been most active on FB and it\u2019s not saying much since I\u2019ve posted continuously for the last 90 days. But the darker shades of red do indicate larger spurts of creative posts (Context\u200a\u2014\u200aI post original content on my feed\u200a\u2014\u200apuns, doodles ; I will not comment on the quality though, out of sheer modesty. Ah, meta irony.).</p> <p>It also allows me to already validate that which I know\u200a\u2014\u200athe days when I haven\u2019t been active on Facebook at all. The relatively large expanse of empty land in the first half of the snapshot above indicates my brief non-usage of the social networking time-waster.</p> <p>But I do not begrudge it. It has provided me with an avenue to practice and expand my data visualisation (and eventually, hopefully prediction) learnings.</p>"},{"location":"blog/2017/11/27/data-visualization-exp2/#notes-potholes-additional-learnings","title":"Notes, potholes, additional learnings \u2014","text":"<ul> <li> <p>\"Not every API calls end up good. Some of them go bad\" : I hit some parsing errors when I obtained a 401 because the id I was using was due to posts by friends on my timeline. I ended up filtering calls only on my id after that and validated for 200 OK status codes, only following which parsing should be done.</p> </li> <li> <p>Practicality: This study can potentially be extended to a Digital Marketing use-case where marketers can figure out when they have been most active on their page, as opposed to their profile._</p> </li> <li> <p>Learning: Heatmaps are cool. But it would be cooler to be able to click on each segment and expand it to get more insights per day. I don\u2019t know how to do that yet. I\u2019ll learn and jot it here in a future experiment.</p> </li> </ul>"},{"location":"blog/2017/11/27/data-visualization-exp2/#whats-next","title":"What\u2019s next?","text":"<ul> <li> <p>The heatmap I\u2019ve generated and the link for which I\u2019ve pasted below takes around 45 friggin\u2019 seconds to load up because of the amount of data it\u2019s loading. I need to read up on caching and loading the page to speed it up. I know I\u2019ll figure it out eventually anway but tips from professionals would be most welcome.</p> </li> <li> <p>Hopefully, there will be no more deviations and I\u2019ll end up actually mining and visualising \u201cthat which I originally intended\u201d.</p> </li> </ul>"},{"location":"blog/2017/11/27/data-visualization-exp2/#reference-material","title":"Reference material \u2014","text":"<p>https://developers.facebook.com/tools-and-support/ https://github.com/DKirwan/calendar-heatmap</p>"},{"location":"blog/2017/11/27/data-visualization-exp2/#source-code-and-other-intermediate-data","title":"Source code and other intermediate data \u2014","text":"<p>Github Link</p> <p>I\u2019ll expand on the Readme file soon to indicate which files need to be focussed upon in case someone\u2019s interested in contributing. Suffice to say for now that \u201cReactions1.ipynb\u201d, \u201cPostprocessing.ipynb\u201d and the FB-FrontEnd folder are what are chiefly at play here.</p> <p>Closer look at the heatmap\u200a\u2014\u200aHere (You'll have to wait for 45 seconds for the entire graph to load :-/ Hey, you wait 2 whole minutes for popcorn, don\u2019t you?)\u200a\u2014\u200aSeriously though, needs speed-up.</p>"},{"location":"blog/2017/11/27/data-visualization-exp2/#also-published-in","title":"Also Published In \u2014","text":"<p>Hacker Noon</p> <p>Thanks for reading!</p> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2018/06/26/django-filter-to-shorten-naturaltime/","title":"Django Filter To Shorten Naturaltime","text":"<ul> <li>This is a filter snippet to shorten the natural time value obtained using naturaltime function from humanize.</li> </ul> <pre><code>#In templatetag file , say file_tags.py\nfrom django import template\nregister = template.Library()\n\n@register.filter\ndef shorten_naturaltime(naturaltime):\n    naturaltime = naturaltime.replace('minutes','m').replace('hours','h').replace('days','d')\n    naturaltime = naturaltime.replace('months','mon').replace('weeks','w').replace('week','w')\n    return naturaltime\n\n#In Usage file\n{% load file_tags %} \n{{start_time|naturaltime|shorten_naturaltime}} #where start_time is the datetime object to be modified\n</code></pre> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2021/12/27/enabling-spark-history-server-standalone/","title":"Enabling Spark History Server Standalone","text":"<p>I've been using Spark for close to 2 years now but because I've always used it largely on clusters at work, I've never really had to struggle with the minutae of enabling monitoring pages like the Spark History Server UI etc. </p> <p>Now that I'm exploring some advanced concepts in Spark, one of the first things I learnt was enabling the Spark History Server locally i.e on a Standalone installation. </p> <p>Let's assume that our Spark installation's version is 2.3.2 and we've extracted the binary into /opt/spark-2.3.2-bin-hadoop2.7.</p> <p>By the name itself, it's evident that the Spark History Server is a...server. In that, it can be served as a webapp of its own and can be accessed at a particular port. It can be served from the sbin folder of spark-2.3.2-bin-hadoop2.7. But before we can do this, a couple of config values need to be set. </p> <ul> <li>In <code>/opt/spark-2.3.2-bin-hadoop2.7/conf</code>, we initially have a spark-defaults.conf.template. Make a copy of that and rename it an actual config file like so - </li> </ul> <p><code>cp spark-defaults.conf.template spark-defaults.conf</code></p> <ul> <li> <p>Next we'll create a folder called \"spark-events\" in a folder where we want to store the logs of the spark jobs we'll be executing. In this case, that folder is <code>/opt/spark-2.3.2/logs/</code>. Now <code>/opt/spark-2.3.2-bin-hadoop2.7/logs/spark-events</code> is in place. </p> </li> <li> <p>Next we'll go back to the <code>spark-defaults.conf</code> file we created and add three lines in it -</p> </li> </ul> <pre><code>spark.eventLog.enabled           true  \nspark.eventLog.dir               file:/opt/spark-2.3.2-bin-hadoop2.7/logs/spark-events  \nspark.history.fs.logDirectory    file:/opt/Softwares/spark-2.3.2-bin-hadoop2.7/logs/spark-events\n</code></pre> <ul> <li> <p>Finally, we start the spark-history-server as <code>/opt/spark-2.3.2-bin-hadoop-2.7/sbin/start-history-server.sh</code></p> </li> <li> <p>Any errors occurring in this process can be looked up in the history server logs under the new logs folder we just created. </p> </li> </ul> <p>It can be accessed at <code>localhost:18080</code> and it should look like this - </p> <p></p>"},{"location":"blog/2019/01/09/f-u-python-commands/","title":"F U Python Commands","text":"<p>The previous post was for commands Unix. This post is about the Python commands I use or Google frequently.</p> <p>1) To find the absolute path of the directory of the current file being executed -</p> <pre><code>abs_path = os.path.dirname(os.path.realpath(__file__))\n</code></pre> <p>2) To copy a file from source to destination (using shutil)-</p> <pre><code>from shutil import copyfile\n  copyfile(src, dst)\n</code></pre> <p>3) To copy a file or directory from source to destination (using shutil)-</p> <pre><code>from shutil import copy\n  copy(src, dst)\n</code></pre> <p>4) To create a directory if it doesn't exist -</p> <pre><code>import os\nif not os.path.exists(directory):\n    os.makedirs(directory)\n</code></pre> <p>5) To write a JSON to a file -</p> <pre><code>import json\nwith open('data.json', 'w') as f:\n    json.dump(data, f)\n</code></pre>"},{"location":"blog/2019/10/31/git-jupyterlab-ext/","title":"Git Jupyterlab Ext","text":"<p>Disclaimer : Extensions in Jupyter-lab are still very much experimental. But this one seems to be working fabulously so far.</p> <p><code>jupyterlab-git</code> is an extension that lets you stage and commit changes to notebooks made right from within the Jupyterlab interface.</p> <p>This is best installed within the confines of a virtual environment, as is anything experimental.</p>"},{"location":"blog/2019/10/31/git-jupyterlab-ext/#installation-steps-","title":"Installation steps -","text":"<p>a) Ensure that you have the latest version of jupyterlab (=1.2.0 at the time of this writing) and NodeJS(=12.13.0 at this time)</p> <pre><code> pip install -U jupyterlab\n</code></pre> <p>b) Install jupyterlab-git</p> <pre><code> pip install --upgrade jupyterlab-git\n</code></pre> <p>c) Build jupyter</p> <pre><code> jupyter lab build\n</code></pre> <p>d) Enable jupyterlab_git</p> <pre><code> jupyter serverextension enable --py jupyterlab_git --sys-prefix\n</code></pre> <p>e) Run jupyter-lab</p> <pre><code> jupyter-lab\n</code></pre> <p>In Jupyter-lab</p> <p>f) Enable the Extensions Manager (experimental)</p> <p></p> <p>g) Search for and Install jupyterlab-git extension. You'll be asked to Rebuild Jupyter.</p> <p></p> <p>h) Once the Git extension shows up post rebuilding, you can stage any changes made per notebook.</p> <p></p> <p>i) Commit post-staging</p> <p></p> <p>j) Push!</p> <p></p> <p>If you see this, you're successfully done :)</p> <p></p>"},{"location":"blog/2019/10/31/git-jupyterlab-ext/#additional-reference-links-","title":"Additional reference links -","text":"<ul> <li>https://github.com/jupyterlab/jupyterlab-git</li> <li>Looking through the issues help a lot!</li> </ul> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2019/08/16/ml-interpretability/","title":"ML Interpretability","text":"<p>I attended an interesting Webinar titled \"A Data Science Playbook for Explainable ML/AI\" conducted by Chief Data Scientist,Josh Poduska, and VP of Marketing, Jon Rooney of Domino Data. This post lays out some of the highlights (in my opinion) of the talk, peppered with my understandings and some additional points that might be of interest. I will breaking this learning-post up into two parts - the first part discussing the theoretical concepts of interpretability and Part-2 on different Open Source models that are in practice today. This is Part-1.</p> <p></p> <p>The first question they started off with was probably one that was on everyone's mind - <code>Why care about Model Interpretability?</code>. The reasons they provided were threefold - - Model ethics, bias and misuse - Regulatory requirements - Trust and understanding</p> <p>Apart from the 2nd reason that most models will need to be regulated from a corporate standpoint soon, it's not hard to see why it's intuitive that models that can be understood logically internally, are easier to trust than a blackbox model that just spits out results. It's also easy to spot bias if the model's working can be understood.</p> <p>Model Interpretability can broadly be bifurcated into two categories -  <code>Global Interpretability</code> &amp; <code>Local Interpretability</code>.</p> <p>The example presented to illustrate the meaning of Global and Local interpretability was that of the purchase of multi-game tickets next year and to try and see which model would help predict this best.  <code>(1)</code> A model that would help pick out almost all the people that would purchase multi-game tickets next year.  <code>(2)</code> A model that told you what combination of characteristics of a buyer are strongly associated with multi-game purchases. <code>(3)</code> A model that told you why it thinks why a particular person would or would not purchase multi-game tickets.</p> <p>- In <code>Model (1)</code>, you get a high level of accuracy in that you get exactly what you want - the entire set of individuals that are likely to purchase a multi-game ticket and once you get this and it works, you don't really care about how this works, because it works. There is no interpretability here. - <code>Model(2)</code> is an example of one with global interpretability. This is because you're able to justify to external stakeholders why your model tells you a group of buyers will or will not buy a multi-game ticket based on the entire dataset. Examples of predictions made by this model are \"In general, people who purchased cheap seats in advance are unlikely to purchase a multi-game ticket\" or \"People who made a last-minute purchase are likely to buy a multi-game ticket.\" - <code>Model(3)</code> is indicative of local interpretability because it might tell you that a certain individual 'X' is likely to purchase a ticket with a probability of 0.82 because of contributing factors/features like seat location (38%), the fact that he purchased the ticket 3 days in advance (22%), his age (17%) and because he spent more on concessions (5%).</p> <p>Models can therefore be <code>predictive</code> or <code>explanatory</code> or both. If it's a case where you care more about immediate, accurate results than about explaining what's going on, you just want a predictive model (for e.g. Language translation). On the other hand, a medical prediction probably deserves both accuracy and explanations on how the predictions were made. More real-world usecases of where they're respectively applicable are shown in the table below -</p> <p></p> <p>In mathematical terms, let there be a set of inputs <code>X</code>, that cause a response <code>Y</code> via a function <code>F</code>, such that <code>Y=F(X)</code>. Then if <code>x</code> and <code>y</code> are samples from X and <code>Y</code> respectively, and <code>f</code> is a model that approximates <code>F</code>, then -  - Interpretable modeling seeks an <code>f</code> that best explains <code>F</code>  - Predictive modeling seeks an <code>f</code> that best predicts <code>y</code>.</p> <p>Ideally, we'd like to find a model <code>f</code> that explains <code>F</code> and predicts <code>y</code> but that is a practical impossibility, therefore \"tradeoffs must be made\".</p>"},{"location":"blog/2019/08/16/ml-interpretability/#oldy-but-goody-techniques-that-offer-localglobal-interpretability","title":"\"Oldy-but-goody\" Techniques that offer local/global interpretability","text":"Technique Local Global Linear Regression (prediction) :heavy_check_mark: - Fully specified model (coefficients) - :heavy_check_mark: Response Surface :heavy_check_mark: - Prediction Surface - :heavy_check_mark: Point Prediction Error Bands :heavy_check_mark: - Partial Dependency Plots - :heavy_check_mark: Variable Importance Plots (from Random Forest/XGBoost) - :heavy_check_mark: <p>I didn't quite understand their explanation wrt the first two rows. I'll need to read more to justify the difference between them.</p> <p>This is where we'll mark the logical end of this part of the post. The next part will discuss Open Source tools in practice and how effective they are w.r.t ML Interpretability - e.g. SHAP, LIME, InterpretML etc.</p>"},{"location":"blog/2019/08/16/ml-interpretability/#additional-notes","title":"Additional notes : <ul> <li>Interpretability of models, as we've seen is very critical in good Machine Learning practice, but what is also important is <code>reproducibility</code> (which is something of a by-product of the former but needs addressing.). This helps in increased accountability and also as a practice to be able to obtain consistent results across repeated experiments. A good case for Google Colab Notebooks to this end is made in this talk -  https://slideslive.com/38915883/how-can-reproducibility-support-understanding</li> </ul>","text":""},{"location":"blog/2019/08/16/ml-interpretability/#links-references-that-might-help-in-understanding-ml-interpretability-and-related-concepts-better-","title":"Links &amp; references that might help in understanding ML Interpretability and related concepts better - <ul> <li>https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning</li> <li>https://www.kdnuggets.com/2018/06/human-interpretable-machine-learning-need-importance-model-interpretation.html</li> <li>https://www.kaggle.com/dansbecker/partial-dependence-plots</li> </ul>","text":""},{"location":"blog/2019/08/16/ml-interpretability/#source-for-the-meat-of-the-matter-wrt-definitions-images-etc-","title":"Source for the meat of the matter wrt definitions, images (etc) - <ul> <li>https://www.brighttalk.com/webcast/17563/366621</li> </ul> <p>Here's TL;DR version of this post - </p> <p>{% if page.comments %}</p>   Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>","text":""},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/","title":"Memory Monitor Widget Using Python","text":"<p>I use Ubuntu 16.04 and I've been searching for a good memory monitoring desktop widget to no avail. I found a couple of taskbar widgets but none that would tell me the free memory left in addition to the used/swap/cache memories. </p> <p>So I decided to write my own (with loads of help from the Internet of course).</p>"},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/#component-1-the-widget","title":"Component 1 \u2014 The widget","text":"<p>I didn't know how to do this. Thankfully Stuart Langridge had already done this for me. I tweaked the <code>override_background_color</code> parameter to get a red background for my widget and the size of the widget itself to be <code>400*20</code> so it would be a nice small strip rather than a big blob of a rectangle.</p>"},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/#component-2-memory-tracking","title":"Component 2 \u2014 Memory tracking","text":"<p>As is common knowledge, one can find out the current memory status using <code>free</code> on Ubuntu. Going a little further, <code>awk '/^Mem/ {print $4}' &lt;(free -h)</code> gives me just the 'Mem' row of the complete information. The values in this row are the Total memory, Used memory, Free memory, shared Memory, Cache memory and the Available memory respectively. I'm personally concerned only with the amount of Free Memory but the rest of the information doesn't hurt. </p>"},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/#setup","title":"Setup","text":"<ul> <li>After you clone the repo, you'll have to create an empty file called <code>memory.html</code> in the same directory. </li> <li>For now, the way I'm using these scripts is by running them in terminal in the background(&amp;)  I'm sure there are a lot of other ways to automate the entire process and run this - I'll look into them. I monitor it every 3 seconds. And no, the scripts themselves don't consume much memory :D </li> </ul>"},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/#result","title":"Result","text":"<p>This is how it looks now : a very just-usable desktop widget. No frills. It gets updated every 3 seconds.</p> <p></p>"},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/#to-do","title":"To-Do","text":"<ul> <li>Label the values</li> <li>Better UI</li> <li>Test and adapt this across other operating systems. Although I'm sure other OS' have their share of awesome widgets. </li> </ul>"},{"location":"blog/2018/06/05/memory-monitor-widget-using-python/#source-code","title":"Source Code \u2014","text":"<p>https://github.com/abhiramr/Everything_Python/tree/master/Random_Py_scripts/Desktop_Widgets/Memory%20Monitor</p> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2018/12/21/my-hipc-2018-experience/","title":"My HIPC 2018 experience","text":"<p>The HIPC Conference this year (2018) was held at the Radisson Blu hotel in Bangalore, India from 17-20 December. This year, HIPC celebrated its 25th year of running and it was amazing.</p> <p>--WIP--</p>"},{"location":"blog/2018/10/01/nginx-gunicorn-ref-links/","title":"Nginx Gunicorn Ref Links","text":"<p>So, I looked up a lot of links while learning how to configure Gunicorn and Nginx for a Flask app I'm building. An article on the specifics of what I learnt will follow this one. This one focuses on the links and resources themselves -</p>"},{"location":"blog/2018/10/01/nginx-gunicorn-ref-links/#first-of-all-youre-probably-asking-why-do-i-need-gunicorn-and-nginx-thats-what-i-looked-up-first-as-well","title":"First of all, you're probably asking - Why do I need Gunicorn and Nginx? That's what I looked up first as well.","text":"<ul> <li>https://serverfault.com/questions/331256/why-do-i-need-nginx-and-something-like-gunicorn</li> </ul>"},{"location":"blog/2018/10/01/nginx-gunicorn-ref-links/#next-how-to-actually-go-about-the-whole-process","title":"Next, how to actually go about the whole process?","text":"<p>The obvious links: - https://gunicorn.org/#quickstart - https://www.linode.com/docs/web-servers/nginx/how-to-configure-nginx/ - http://nginx.org/en/docs/beginners_guide.html - https://www.fullstackpython.com/green-unicorn-gunicorn.html</p> <p>Quora is generally useless nowadays but this was as a proper answer: - https://www.quora.com/What-are-the-differences-between-nginx-and-gunicorn/answer/Pramod-Lakshmanan</p> <p>The architecture of Nginx is very well explained here: - http://www.aosabook.org/en/nginx.html</p> <p>This link was really helpful - https://realpython.com/kickstarting-flask-on-ubuntu-setup-and-deployment/</p> <p>Some Stackoverflow answers that helped: - https://stackoverflow.com/questions/11061788/correct-configuration-for-nginx-to-localhost - https://stackoverflow.com/questions/43044659/what-is-the-purpose-of-using-nginx-with-gunicorn - [https://stackoverflow.com/questions/30165746/nginx-return-301-vs-rewrite])https://stackoverflow.com/questions/30165746/nginx-return-301-vs-rewrite</p> <p>Other useful links: - https://medium.com/ymedialabs-innovation/deploy-flask-app-with-nginx-using-gunicorn-and-supervisor-d7a93aa07c18 - https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04 - https://www.digitalocean.com/community/tutorials/how-to-move-an-nginx-web-root-to-a-new-location-on-ubuntu-16-04 - https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-16-04 - https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-18-04 - https://www.agiliq.com/blog/2013/08/minimal-nginx-and-gunicorn-configuration-for-djang/ - https://gist.github.com/netpoetica/5879685</p>"},{"location":"blog/2018/10/01/nginx-gunicorn-ref-links/#whats-next","title":"What's next?","text":"<p>I want to hook up icinga to see if it's going to serve my monitoring purpose. I don't know anything about Icinga at the time of writing this note -</p> <ul> <li>https://www.digitalocean.com/community/tutorials/how-to-use-icinga-to-monitor-your-servers-and-services-on-ubuntu-14-04</li> </ul>"},{"location":"blog/2018/10/01/nginx-gunicorn-ref-links/#note-","title":"Note -","text":"<p>This link I accidentally found in the process explaining architectures of a lot of Open Source systems. This is pretty awesome.</p> <ul> <li>http://www.aosabook.org/en/index.html</li> </ul> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2019/08/23/pandas-notes/","title":"Pandas Notes","text":"<p>These are the commands I use a lot using Pandas - </p> <p>Get rows with NaNs in any column </p> <p><code>df[df.isna().any(axis=1)]</code></p> <p></p> <p>Get dataframe excluding all NaNs in a particular Column </p> <p><code>df[df['&lt;col_name&gt;'].notnull()]</code></p> <p></p> <p>Get counts of number of Nulls per column</p> <p><code>df.isnull().sum()</code></p> <p>Increase width of columns displayed in Jupyter Notebooks - This is possibly one of my most used commands for every notebook that involves dataframes.</p> <p><code>pd.set_option('display.max_colwidth',400)</code></p>"},{"location":"blog/2019/11/07/setting-up-python38-and-jupyterlab/","title":"Setting Up Python3.8 and Jupyterlab","text":"<p>I'm using Ubuntu 16.04 on this machine, so that's what the steps of installation here will be for. But this installation shouldn't largely vary on any distro that's 14.04 and higher.</p>"},{"location":"blog/2019/11/07/setting-up-python38-and-jupyterlab/#step-1-download-python-source-code","title":"Step 1 - Download Python source code","text":"<p>Binary installations are available for Windows and Mac but you can either install Python3.8 on Ubuntu using \"apt\" or download and install from source. I did the latter.  Obtain the gz file from here.</p> <pre><code> cd &lt;path-of-archive&gt;\n tar zxvf Python-3.8.0.tgz\n cd Python-3.8.0\n ./configure\n make\n sudo make altinstall\n</code></pre> <p>We use <code>make altinstall</code> instead of <code>make install</code> so that your existing Python configuration not be disturbed.</p> <p>This process should likely complete in under a minute.</p> <p>Confirm that it has successfully installed -</p> <pre><code> python3.8 \n</code></pre> <p><code>(should return the IDLE prompt with Python version 3.8.0)</code></p>"},{"location":"blog/2019/11/07/setting-up-python38-and-jupyterlab/#step-2-install-python38-specific-virtualenv","title":"Step 2 - Install python3.8 specific virtualenv","text":"<p>There are a few differences in Python3.8 that prevent your existing virtualenv from being useful in creating a virtual environment supporting this version of Python. So install virtualenv corresponding to Python3.8.</p> <pre><code> sudo pip3.8 install virtualenv\n</code></pre> <p>This should likely be installed in <code>/usr/local/bin/</code>. So, confirm that it exists -</p> <pre><code> /usr/local/bin/virtualenv --version \n</code></pre> <p><code>(should return the latest version of virtualenv = 16.7.7 as of this writing)</code></p>"},{"location":"blog/2019/11/07/setting-up-python38-and-jupyterlab/#step-3-create-a-virtual-environment","title":"Step 3 - Create a virtual environment","text":"<p>Use the installation of virtualenv created in <code>Step 2</code> to create a virtual environment with the Python path pointed to python3.8 (-p). Preferably create a folder for all your virtual environments if you don't have a preferred directory already.</p> <pre><code> cd\n mkdir .virtual_environments\n cd .virtual_environments\n /usr/local/bin/virtualenv -p python3.8 &lt;name of environment&gt;\n</code></pre> <p>Example - <code>/usr/local/bin/virtualenv -p python3.8 py38</code></p>"},{"location":"blog/2019/11/07/setting-up-python38-and-jupyterlab/#step-4-activate-this-environment-and-install-jupyterlab-in-it","title":"Step 4 - Activate this environment and install jupyterlab in it.","text":"<p><code>cd</code> to the folder where you created the virtual environment. Using the environment we created in Step 3 - </p> <pre><code> cd ~/.virtual_environments\n source py38/bin/activate\n pip list (You should see only a few packages in your new environment.)\n pip install jupyterlab\n</code></pre> <p>Now you can traverse to the directory where you want your Jupyter-lab notebook to be spawned and trigger it using <code>jupyter-lab</code> and the default kernel (if you don't have any other kernel installed) that will be picked up will be running Python3.8.</p> <p></p>"},{"location":"blog/2019/11/07/setting-up-python38-and-jupyterlab/#fin","title":"Fin.","text":"<p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2018/02/12/studying-the-show-about-nothing/","title":"Studying The Show About Nothing","text":"<p>I have watched and re-watched Seinfeld over and over for the last 6\u20137 years now and it remains, till date, one of my most favorite shows. For a show that claims to be about nothing, it has certainly managed to fill up nine seasons worth of content, all with seemingly no plot.</p> <p>The show, if you\u2019re unfamiliar with it, traces the day-to-day lives of (mainly) 4 quite unremarkable people\u200a\u2014\u200aJerry Seinfeld, George Costanza, Cosmo Kramer and Elaine Benes. These 4, along with plenty of other side characters and guest stars engage in what seems to be a series of meaningless conversations and activities, which somehow all come together towards the end of the duration of each episode.</p> <p>Yesterday while animatedly watching one of the episodes in which Frank Costanza talks about the a festival he had created, \u201cFestivus\u201d, I found myself thinking how much I adored Frank Costanza as a character. But he hadn\u2019t featured in many episodes right? Neither had Jackie Chiles, the versatile lawyer, and yet he\u2019d had the same effect as had Frank. Which episodes were those again? Damn. You\u2019d thinking having watched this show so many times, such information would be \u201ctattooed in my brain\u201d. But admittedly, it was not. And there Had to be a more complicated, yet fun way to get this information than looking it up on Wikipedia. And thus, the weekend data-dive game was on.</p> <p>The measure used in this analysis is the number of times a person has a line to speak per episode. So it follows that the first time a person (say Jackie Chiles) had a line throughout the show would be the first time he/she was featured. Other results that followed were how important a character was to the show or an episode, or in how many episodes was say, Elaine or Kramer, not present. Also, we know the central 4 characters, but how do the 3 characters apart from Jerry rank against each other? So, this is how I went about it.</p>"},{"location":"blog/2018/02/12/studying-the-show-about-nothing/#scraping-and-parsing-the-data","title":"Scraping and parsing the data.","text":"<p>I obtained the list of episodes and scraped every episode\u2019s script off of seinfeldscripts.com  using Requests at a consistent rate so as to not overwhelm them, even if it was just around 200 automated API calls. This website also has the listing of all episodes with the episode numbers from 1\u2013180 alongside it.[1] <p>The data scraped was neither very manageable nor consistent in the form obtained. So a pattern was slightly troublesome to derive, but finally I found a way to just get the dialogues and more importantly, the person who delivered them using Beautiful Soup (bs4).</p> <pre><code>soup = BeautifulSoup(req.content, \u2018html.parser\u2019)\nparagraphs = soup.find_all('p')\n\n.\n.\n.\n\nif (j.isupper() and len(j)&gt;1 and \u2018:\u2019 in j):\n     if j in chars_dict:\n         chars_dict[j]=chars_dict[j]+1\n         chars_dict[\u2018TOTAL\u2019]=chars_dict[\u2018TOTAL\u2019]+1\n     else:\n         chars_dict[j]=1\n         chars_dict[\u2018TOTAL\u2019]=chars_dict[\u2018TOTAL\u2019]+1\n</code></pre> <p>In case it was a well written script, the pattern would be something like - KRAMER: \u201cHoochiemama!\u201d</p> <p>And the above mentioned code snippet would handle. But in many of the files you had entries like - George: \u201cVandelay Industries! Say Vandelay Industries\u201d</p> <p>And the only indication here that a dialogue has begun is the colon (\u201c:\u201d). So this was the other case to handle.</p>"},{"location":"blog/2018/02/12/studying-the-show-about-nothing/#storing-and-filtering-the-data","title":"Storing and filtering the data","text":"<p>Looping through the entire set of scripts resulted in a dictionary of dictionaries, where the key was an episode number and the value was a dictionary of the cast for that episode with their corresponding counts of dialogues.</p> <p>Next I consider the characters I\u2019m primarily interested in, since for this usecase, I don\u2019t really care about some people like Sue Ellen Mischke or Stan the Caddy.</p> <pre><code>characters =[\u201cJERRY\u201d,\u201dGEORGE\u201d,\u201dELAINE\u201d,\u201dSUSAN\u201d,\u201dKRAMER\u201d,\u201dJACKIE\u201d,\u201dFRANK\u201d,\u201dRAVA\u201d,\u201dNEWMAN\u201d,\u201dTOTAL\u201d]\n</code></pre> <p>After translating the keys of the dictionary obtained in Step 3 to uniformly be in upper case, I store the occurrence values per episode for the characters mentioned above along with the total number of dialogues per episode (\u201cTOTAL\u201d) in a dataframe since Pandas is a very good tool to manipulate data.</p> <p>The occurrence values were then normalized to be in percentages of the TOTAL.</p> <p>Despite all the attempts to obtain complete data, there were some files i.e. episode scripts that were able to be only partially processed or were unable to be processed because there was no way to distinguish beyond a shadow of the doubt between the character names and the dialogues themselves. This was obviously because the only source considered was Seinfeldscripts.com. A future experiment will probably consider other sources to fill in the missing data. To this end, I\u2019ve filtered out entries that had a TOTAL value &lt; 100</p> <pre><code>df = df[df.TOTAL &gt; 100].sort_values(by=\u201depisode_num\u201d)\n</code></pre> <p>The finale episodes have also been omitted from this analysis.</p>"},{"location":"blog/2018/02/12/studying-the-show-about-nothing/#results","title":"Results:","text":"<p>The main result was a set of graphs based on the following criteria I chose -</p> <ol> <li>% of dialogues per episode of Elaine vs Kramer</li> <li>% of dialogues per episode of Elaine vs George</li> <li>% of dialogues per episode of George vs Kramer</li> <li>Episodes in which Frank Costanza featured </li> <li>Episodes in which Susan featured</li> <li>Episodes in which Jackie Chiles featured</li> <li>Episodes in which Newman featured</li> </ol> <p>Other miscellaneous graphs were also computed.</p> <p>The X-axis for all graphs shows the episode number (albeit unclear) and the Y-axis tracks the % occupied per episode.</p> <p></p> <p>Understandably, Jerry has the largest number of dialogues in every episode.</p> <p>1. Elaine vs George :</p> <p></p> <p>The trend shows that George (~20\u201330%) always has more screen space than Elaine(~5\u201325%) .</p> <p>2. Elaine vs Kramer :</p> <p></p> <p>While they seem to be uniformly matched for the most part, it looks like Elaine almost always has more to say than Kramer, atleast for the first 60% of the total number of episodes. Towards the end however, Kramer seems to be having more to say.</p> <p>3. George vs Kramer:</p> <p></p> <p>No surprises here. George dominates with ~20\u201330% of the conversations while Kramer averages out at close to 15% of the conversations.</p> <p>So, we see that in terms of main character importance, Jerry &gt; George &gt; (Elaine = Kramer). Moving on,</p> <p>4. Frank Costanza:</p> <p></p> <p>Frank Costanza first appears in episode 62\u200a\u2014\u200a\u201cThe Handicap Spot\u201d and finally in episode 166\u200a\u2014\u200a\u201cThe Strike\u201d (the legendary Festivus episode) barring the finale episodes.</p> <p>5. Susan Ross</p> <p></p> <p>Susan first appears in episode 43\u200a\u2014\u200a\u201cThe Pitch\u201d and finally in episode 134\u200a\u2014\u200a\u201cThe Invitations\u201d, when\u2026 you know..</p> <p>6. Jackie Chiles</p> <p></p> <p>Jackie Chiles plays a major role in only 3 episodes Episodes 113, 122 and 143, again barring the Finale. I finally have my answer :D</p> <p>7. Newman</p> <p></p> <p>We see that Newman plays a key role in more episodes than any of the other side characters, going upto 14% in episode 132\u200a\u2014\u200aThe Bottle Deposits\u200a\u2014\u200apart 2 where he runs a bottle scam with Kramer (Aah nostalgia).</p> <p>This experiment was a lot of fun. I hope to fill in the blanks in the next ones and also work out stats and relationships for the other characters.</p>"},{"location":"blog/2018/02/12/studying-the-show-about-nothing/#serenity-now","title":"SERENITY NOW!!!!","text":"<p>[1] http://www.seinfeldscripts.com/seinfeld-scripts.html</p> <p>[2] Code : https://github.com/abhiramr/DataViz/blob/master/Seinfeld/Seinfeld_Analysis.ipynb</p> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2018/03/06/surprising-searches-1-of-n/","title":"Surprising Searches 1 of n","text":"<p>You\u2019re probably aware of most of the products that Google offers. Trends, is one of them and it allows you to look up various trends of searches by time, country etc. It\u2019s a lot of fun\u2026if you\u2019re into that sort of thing.</p> <p> </p> <p>I was randomly searching for usages of English words on Google Trends (as one does) and as I went through words of no real significance\u200a\u2014\u200aboat, silo etc, at one point paused and almost on a whim looked up \u201cBatman\u201d , juust to see if people search for knowledge about the caped crusader on a daily basis.</p> <p>Turns out they did, for reasons I have not yet looked up -</p> <p> </p> <p>As we can see, there\u2019s a steady graph rising and falling unsurprisingly. There\u2019s a weird spike during the Jul 11\u201317 period. But that\u2019s not the point of this post.</p> <p>I scrolled further down on the Trends page and came across the country stats section, and something here stood suspiciously out.</p> <p> </p> <p>There has been some curiosity around the Americas and some in Australia, but what jumps out is the glaring number and deep intensity of blue on the chart from Turkey, of all places! I wondered why this was and it turns out there is a city named Batman in Turkey!!! Yes. I got excited as well. Surely a city named for (I thought) one of the best DC characters ever created has a whole bunch of Batman related things to see and do there?! Sadly, there isn\u2019t. There isn\u2019t even a correlation between the names. The city was renamed from its original name of Iluh to Batman purely for the river Batman that flowed through it. The river itself was named for a then-common unit of mass used by the Ottoman Empire in Turkey which was \u201cbatman\u201d (Turkish pronunciation: [bat\u02c8man]).</p> <p>So, there definitely is no connection\u2026 :( except for a lawsuit that was threatened to be filed by the Mayor of Batman against Warner Bros in 2008 after The Dark Knight was released, but nothing happened there.</p> <p>Needless to say, this place has always been a topic of discussion among tourists, but I happened to chance upon this little nugget of information just today and I\u2019m sure there are more things to glean from just simple searching and the intuitive that visualization offers .</p> <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"blog/2018/12/24/unix-commands-i-use/","title":"Unix Commands I Use","text":"<p>I keep Googling commands for some of these situations regularly -</p> <p>General -</p> <p>1) To find files one level below your current directory - </p> <p>I was trying to find which repos in my set of folders have a setup.py file. So, this is what I'd use.</p> <p><code>find . -maxdepth 2 -iname setup.py</code></p> <p>2)  To find what process is running on port 2181 - </p> <p><code>sudo lsof -n -i :2181 | grep LISTEN</code></p> <p>3) Reduce prompt width on bash - </p> <p>If the current directory you're in is very deep in the directory tree, chances are, your System Prompt is pretty long and this wastes a lot of screen space. Starting from bash v4 though, there's a new variable called <code>PROMPT_DIRTRIM</code> that you can set.</p> <ul> <li> <p>Before : </p> </li> <li> <p>After: </p> </li> </ul> <p>Github -</p> <p>4) To change your git remote-url -</p> <p><code>git remote set-url origin &lt;url_name&gt;</code></p> <p>5) Steps for deleting a branch:</p> <ul> <li>for deleting the remote branch: <code>git push origin --delete &lt;your_branch&gt;</code></li> <li>for deleting the local branch: <code>git branch -D &lt;branch_name&gt;</code></li> </ul> <p>6) To delete a file from remote while retaining your local copy -</p> <ul> <li>For single file: <code>git rm --cached mylogfile.log</code></li> <li>For single directory: <code>git rm --cached -r mydirectory</code></li> </ul>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/","title":"Learning how to use FFMpeg","text":""},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#what-is-ffmpeg","title":"What is FFmpeg?","text":"<p>In their own words, FFmpeg is \"A complete, cross-platform solution to record, convert and stream audio and video.\"</p> <p>I've been recording Bangpypers videos for the last few months and I haven't really had access to a proper solution to edit videos and audio that I could have got for free/ low-cost. A good friend of mine, Vinay Keerthi, (who incidentally presented the webinar under discussion) told me to chuck GUI based fronts for editing them and told me to try FFmpeg, the CLI tool directly, which these tools probably use in the background anyway. </p> <p>So I gave it a shot.  And while it isn't very hard to break into, there are a lot of things that aren't very straight-forward. But it's definitely a treat to use and it gives you a sense of satisfaction when things work. </p> <p>tl;dr - I basically just edited a couple of videos using a CLI tool. No biggie. But the potential it opens up for me by way of not having to ever download an illegal copy of a GUI based tool (don't tell anyone I did this) or be restricted by what they offer doesn't have to be the case anymore :) </p>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#my-case-studys-solution","title":"My case-study's solution :","text":"<p>So I've mentioned above about videos recorded for Bangpypers. This particular case today was for editing one such video. </p> <p>Basically, I had a 40 minute webinar that I needed to cut up to replace a part of the video with an image, and then stitch these sections with another video, also a part of the webinar but recorded separately. This second half also needed to have some part of the video trimmed and the rest have the video be replaced with the Bangpypers Logo. </p> <p>There were a lot of experiments I did in the process. </p> <p>For now, here's a working set of steps: </p> <ul> <li>Split the first video from [0,time_a] and [time_a,time_b] resulting in videos say -  video_1_a.mp4, video_1_b.mp4</li> <li>Overlay the video in video_1_a with the desired png = bangpypers.png.</li> <li> <p>Once the overlayed video is available, stitch video_1_a_overlay.mp4 to video_1_b.mp4. = video_1_final.mp4</p> <ul> <li>In this step, there were issues because the image used was a 900*900 png but the video in question was of a higher resolution. So the obvious remedy was to scale the image and stitch it up. But I took some time to get to this resolution and found ways to stitch it directly just fine but the resultant was obviously very messed up. So the entire set of steps had to be repeated. </li> </ul> </li> <li> <p>For the second video, treat it similarly as in the video_1_a.mp4. Let's call this video_2.mp4</p> </li> <li>Trim the video from [time_a,end]</li> <li>Overlay the trimmed video with the desired png = bangpypers.png.</li> <li>Stitch video_1_final.mp4 to video_2_final.mp4</li> </ul>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#usage","title":"Usage:","text":"<ul> <li>You should typically have ffmpeg natively installed if you're using a Mac. If you're on Windows/ Linux, you can download the binary at https://ffmpeg.org/download.html</li> <li>Once you've downloaded the binary, add it to your path if you're on Windows. If you're on Linux, the installation should typically take care of that for you. </li> <li>In this post, I'm using a Mac, so I will be directly using FFmpeg as <code>ffmpeg</code>.</li> </ul>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#commands-for-steps-above","title":"Commands for steps above:","text":"<p>Video 1</p> <p><code>ffmpeg -ss 00:01:01 -i Webinar_Full_Part1.mp4 -t 00:05:19 -c copy video_1_a.mp4</code></p> <p><code>ffmpeg -ss 00:06:21 -i Webinar_Full_Part1.mp4 -t 00:33:50 -c copy video_1_b.mp4</code></p> <p><code>ffmpeg -i video_1_a.mp4 -i bangpypers.png -filter_complex \"[1][0]scale2ref[i][v];[v][i]overlay\" -c:a copy video_1_a.mp4</code></p> <p><code>ffmpeg -i video_1_a.mp4 -i video_1_b.mp4 -filter_complex \"[0:v] [0:a] [1:v] [1:a] concat=n=2:v=1:a=1 [v] [a]\" -map \"[v]\" -map \"[a]\" video_1_processed.mp4</code></p> <p>Video 2</p> <p><code>ffmpeg -ss 00:03:23 -i Webinar_Full_Part2.mp4 -t 00:09:08 -c copy video_2.mp4</code></p> <p><code>ffmpeg -i video_2.mp4 -i bangpypers.png -filter_complex \"[1][0]scale2ref[i][v];[v][i]overlay\" -c:a copy video_2.mp4</code></p> <p><code>ffmpeg -i video_2.mp4 -vf scale=1920:1080,setsar=1:1 video_2_processed.mp4</code></p> <p>Final</p> <p><code>ffmpeg -i video_1_processed.mp4 -i video_2_processed.mp4 -filter_complex \"[0:v] [0:a] [1:v] [1:a] concat=n=2:v=1:a=1 [v] [a]\" -map \"[v]\" -map \"[a]\" Webinar_video_processed.mp4</code></p> <p>The net result of this learning can be viewed at our Bangpypers Channel -  https://www.youtube.com/watch?v=xickNijifOs</p>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#explanation-legend-of-params-used-","title":"Explanation/ Legend of params used -","text":"<ul> <li><code>ss</code>: Start time as hh:mm:ss</li> <li><code>t</code> : Duration for which to clip</li> <li><code>v</code> : Video</li> <li><code>a</code> : Audio</li> <li><code>[0:v] [0:a] [1:v] [1:a] concat=n=2:v=1:a=1 [v] [a]</code> : Concatenating 2 videos (n=2) where both <code>v</code> and <code>a</code> exist. Hence <code>v=1</code>,<code>a=1</code></li> <li>The last parameter in each of the commands is the output video. </li> </ul>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#chronicled-some-more-of-the-commands-here-","title":"Chronicled some more of the commands here -","text":"<ul> <li>https://docs.google.com/document/d/1sax37PO6DMFKCS5iwnl7dboaSnpLRl-YXiY3uyiWhXo/edit?usp=sharing</li> </ul>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#links-referred","title":"Links referred","text":"<ul> <li>https://ffmpeg.org/ffmpeg.html#SEC8</li> <li>https://trac.ffmpeg.org/wiki/Scaling</li> <li>https://itsfoss.com/ffmpeg/</li> <li>https://stackoverflow.com/questions/37327163/ffmpeg-input-link-in1v0-parameters-size-640x640-sar-169</li> <li>https://stackoverflow.com/questions/38753739/</li> <li>https://stackoverflow.com/questions/7333232/how-to-concatenate-two-mp4-files-using-ffmpeg</li> <li>https://stackoverflow.com/questions/40480153/how-to-overlay-place-an-image-on-a-video-in-ffmpeg</li> <li>https://stackoverflow.com/questions/19425674/ffmpeg-concat-and-scale-simultaneously</li> <li>https://superuser.com/questions/377343/cut-part-from-video-file-from-start-position-to-end-position-with-ffmpeg</li> <li>https://superuser.com/questions/855276/join-2-video-file-by-command-or-code</li> <li>https://superuser.com/questions/722247/how-can-i-remove-multiple-segments-from-a-video-keeping-the-audio-using-ffmpeg</li> <li>https://superuser.com/questions/268985/remove-audio-from-video-file-with-ffmpeg</li> <li>https://video.stackexchange.com/questions/20430/how-to-concatenate-multiple-videos-with-ffmpeg?newreg=9c40f3f240a24b30a72120c8cb6f4d76</li> <li>https://video.stackexchange.com/questions/15468/non-monotonous-dts-on-concat-ffmpeg</li> </ul>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#bloopers","title":"Bloopers","text":"<p>I made a lot of mistakes in trying other stuff but learnt quite a bit in the process. Here's a glimpse of the commands tried -</p> <p></p> <p>I hope this helps you if you're looking for help with FFMpeg. Feel free to mail/tweet at  me if you have any issues!</p>"},{"location":"blog/2020/12/06/learning-how-to-use-ffmpeg/#about-bangpypers","title":"About Bangpypers","text":"<p>Bangpypers is one of Bangalore's largest Python User Groups. We conduct Meetups where we have talks and workshops on topics related to Python. Feel free to reach out to mail me in case you want to talk at any of our upcoming sessions. We conduct one every month. More details regarding meetups at https://www.meetup.com/Bangpypers/. We're also on Twitter! <p>{% if page.comments %}</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"misc/","title":"Misc","text":""},{"location":"misc/posts/01_design_of_everyday_things_1/","title":"Notes from Design of Everyday Thoughts by Don Norman - 1","text":"<p>This is the first design-related book I'm reading. I've read blogs in the past though.  I picked this up because of its rave reviews and my own interest in wanting to design software and represent data better. Here I list some notes I made from the book peppered with my own thoughts during the reading. </p>  Chapter 1 : The psychopathology of everyday things  <p>In the reading of this chapter, Don Norman stresses that the 2 most important characteristics of good design are </p> <p> DISCOVERABILITY UNDERSTANDING </p>  What do these terms mean?  <p>Basically, given a product, the intended user should be able to discover all the features the product has to offer and has to be, with minimal-to-zero difficulty understand how to use the product as the designer intended for it to be used. Unfortunately, not many products are designed with these characteristics in mind. :disappointed:</p> <p>If a product is designed well, it results in \"brilliant, pleasurable products.\" And if it's not, the users are forced to behave the way the products want them to and this is counterproductive to the whole reason products are designed - to help people. </p> <p>Of late, a new brand of design is cropping up - HUMAN CENTERED DESIGN, which means exactly what you think - to be designed with a human i.e the end-user in mind. And intuitively, this is how products should have always been designed. But the problem ultimately, as even Don notes is that products are designed, largely, by engineers, who are trained to be logically thinking. But to expect the same of the users they're designing for is unfair. Sometimes blatantly so.  HCD aims at engineers focussing on what happens when things go wrong during the usage of a product and not just when things go all right.</p> <p>One of the best things that can be built into a product (where some form of a display is available) is notes on how to go about resolving the problem when it occurs. This could mean a troubleshooting section or log messages/ non-intimidating user-friendly error messages in case of software. </p> <p>Don proposes 5 fundamental psychological concepts that form the bedrock of Discoverability - </p>"},{"location":"misc/posts/2019-07-08-Podcasts-I-Like/","title":"Podcasts I like","text":"My current regulars <ul> <li>  Mythology by Parcast </li> <li>  Rationally speaking with Julia Galef </li> <li> The Tim Ferriss Show </li> <li> Naval Ravikant </li> <li> Data Stories </li> <li> On Being with Krista Tippett</li> <li> The Python Podcast.__init </li> <li> Akimbo: A Podcast from Seth Godin </li> <li> Outliers</li> <li> Fall of Civilizations</li> </ul> My favorite episodes Literary <ul> <li>Is the catholic church a force for good - An Intelligence Squared Debate - With Stephen Fry and Christopher Hitchens</li> <li>What makes us human? - with Stephen Fry </li> <li>The Four Horsemen - Richard Dawkins, Daniel Dennet, Christopher Hitchens and Sam Harris </li> <li>The Tim Ferriss Show - with Neil Gaiman</li> <li>Neil Gaiman: How Stories Last</li> <li>Stephen Fry on PG Wodehouse</li> </ul> Economics, Psychology, Marketing <ul> <li>The Psychology of Advertising with Rory Sutherland</li> <li>Dan Ariely | Payoff </li> <li>Rationally Speaking #178 - Tim Urban on \"Trying to live </li> </ul> On Data <ul> <li>Data Stories : Color with Karen Schloss</li> <li>FlowingData with Nathan Yau </li> </ul> Python Podcasts <ul> <li>The Real Python Podcast</li> <li>Talk Python To Me </li> <li>Python Bytes </li> <li>Test and Code </li> <li>From Python Import Podcast </li> <li>podcast.init </li> </ul> Podcasts by Friends <ul> <li>Tales of Time Travel by Gautham Shenoy</li> <li>Kannada Science Fiction by Gautham Shenoy</li> <li>The Forgotten Heritage of the Deccan by Karthik Malli</li> </ul> People's Choice <ul> <li>  Simblified </li> <li>  The Pragati Podcast </li> <li> Paperback </li> <li> The Seen and the Unseen </li> <li> Echoes of India </li> </ul> <p>Via Nishant Singh</p> <p>-  The Knowledge Project with Shane Parrish  -  99% Invisible </p> <p>Via Deepak V</p> <ul> <li> Hidden Brain </li> <li> The Moth </li> <li> The Infinite Monkey Cage </li> <li> Quirks and Quarks  </li> <li> The Curious Cases of Rutherford &amp; Fry </li> <li> The Intersection </li> </ul> <p>Via karmanya</p> <ul> <li> Freakonomics Radio</li> <li> Reply All</li> </ul> <p>Via Chaitanya</p> <ul> <li> Invisibilia</li> </ul> <p>Via Preeti</p> <ul> <li> Lore</li> </ul> <p>Via Kriti Monga</p> <ul> <li> Clever - Amy Devers + Jaime Derringer / Design Milk</li> <li> 5x15</li> <li> Design Matters with Debbie Millman</li> <li> Stuff You Should Know </li> </ul> <p>If you'd like to suggest your own favorites to be added to this list, leave them in the comments!</p> Please enable JavaScript to view the comments powered by Disqus. <p>{% endif %}</p>"},{"location":"misc/posts/2019-07-10-Resources-for-kerala/","title":"Resources for Kerala Flood 2019","text":"<p>This page will be updated as and when information is relevantly obtained. </p>"},{"location":"misc/posts/2019-07-10-Resources-for-kerala/#the-following-contact-numbers-and-post-is-obtained-from-ranjith-rajans-facebook-post","title":"The following contact numbers and post is obtained from Ranjith Rajan's Facebook post.","text":""},{"location":"misc/posts/2019-07-10-Resources-for-kerala/#super-urgent","title":"Super Urgent!!","text":""},{"location":"misc/posts/2019-07-10-Resources-for-kerala/#keralaflood2019","title":"KeralaFlood2019","text":"<p>Be alert, but do not panic!!</p> <p>Most of you are now aware, if not closely monitoring the rains at various parts of India, especially Kerala from last night..</p> <p>For Kerala region, we have made a list of database from various sources. This should be helpful for some of them, kindly share!! In case you have any more numbers to be added, please comment on this post and I shall add it to the post!</p> <p>Also, there are several camps in many regions of Kerala already, we shall post the address of relief material collection points shortly to support the camps.</p> <p>Emergency contact number: 112</p> <p>Cochin International Airport Control Room: 4843053500</p> <p>Govt Control Rooms : - Chengannur: 04792452334, 9496231626, 8089106500 - Collectorate control Room: 4862233111, 4862233130 - Udumbanchola Control room: 4868232050  - Devikulam control room: 4865264231  - Peermedu control room : 4869232077  - Thodupuzha control room: 4862222503  - Idukki control room: 4862235361 - Nilambur Emergency help: 9495076823, 9946665656, 9496323582, 949623582, 9447271912</p> <p>Food Supply:</p> <ul> <li>Wayanad : 8921158568</li> <li>Nilambur: 9446407906, ,8304813442, 98461 06108, 94466 32516, 94468 90618, 9495160439</li> </ul> <p>News:</p> <ul> <li>Asianet News: 04714851335</li> <li>Manorama news: 0478 6610200</li> <li>India Vision: 04952304555</li> <li>Media one : 0495 3050500</li> </ul> <p>Sevabharathi helpline: 8330083324</p> <p>Power Supply:</p> <ul> <li> <p>Hello KSEB: 9496010101, 1912</p> </li> <li> <p>Emergency Medical requirement: 8139014696</p> </li> <li>24*7 Ambulance owners Association \u2013 Kozhikode, 9846119666, 9048363688, 9907102102, 9072172100, 9072208108, 9846333108</li> <li>Mavelikara: 9446043242, 9847178851, 9947221590 </li> <li>Disability inclusive disaster risk reduction cell 9446021975 9947922791</li> </ul> <p>Rescue Assistance: </p> <ul> <li>Flood Volunteer family: 9539000541, 7356597834, 8157958454, 9947790008, 9846945397</li> <li>Riseup Forum: 8330096503, 9847863372, 9961927218, 9497866174 </li> <li>World Charity Mission: 9947085689</li> <li>Bangalore Co-ordination 7204280338 9446273252 9986517832 </li> <li>Chennai Coordination 9600115429 8825698484</li> </ul> <p>Assisted rescue: </p> <ul> <li>Wayanad tree clearance: 9961394207</li> <li>Malappuram - Jeep rescue ( KL 10 JEep off roading club)  6282954403, 9847748970, 866845162, 9744318328, 9961233501</li> <li> <p>Malappuram - Rescue Van: 9846300028, 9526212754</p> </li> <li> <p>Drone Assistance:</p> </li> <li>Emergency help 9447456839 9544752580</li> </ul> <p>Rescue - Kerala's own army with Boats </p> <ul> <li>Trivandrum : 0471-2450773,2480 335,9496007026.</li> <li>Kollam - O474-2792850, 9496007027.</li> <li>Pathanamthitta: 0468-2223134,828144 2344.</li> <li>Alappuzha-0477-2251103, 9496007028.</li> <li>Kottayam: 0481-2566823, 9446379027.</li> <li>Idukki: 0486-9222 326, 892 1031800.</li> <li>Ernakulam: 0484-2502768, 9496007029.</li> <li>Thrissur - 0487-244 1132,9496007030.</li> <li>Palakkad: 9074326 046, 9496007050.</li> <li>Malappuram: 0494-2666428, 949600703 1.</li> <li>Kozhikode- 0495-2383780,2414074, 9496007032.</li> <li>Wayanad-0493-6255214, 9496387833.</li> <li>Kannur - 0497-2732487, 9496007033.</li> <li>Kasargod - 0467-2202537, 9496007034.</li> </ul> <p>KSRTC Helpline: - Adoor 0473-4224764  - Alappuzha 0477-2251518  - Aluva 0484-2624242  - Anayara 0471-2749400  - Ankamali 0484-2453050  - Aryanad 0472-2853900 - Aryankavu 0475-2211300  - Attingal 0470-2622202  - Bangalore 0802-6756666  - Chadayamangalam 0474-2476200  - Chalakudy 0480-2701638  - Changanassery 0481-2420245  - Chathannur 0474-2592900  - Chenganoor 0479-2452352  - Cherthala 0478-2812582  - Chitoor 0492-3227488 - Edathuva 0477-2215400  - Eenchakkal 0471-2501180  - Erattupettah 0482-2272230  - Ernakulam 0484-2372033 - Erumely 0482-8212345  - Guruvayoor 0487-2556450  - Harippad 0479-2412620 - Irinjalakkuda 0480-2823990  - Kalpetta 0493-6202611  - Kanhangad 0467-2200055  - Kaniyapuram 0471-2752533  - Kannur 0497-2707777  - Karunagapally 0476-2620466 - Kasaragod 0499-4230677  - Kattakada 0471-2290381 - Kattappana 0486-8252333  - Kayamkulam 0479-2442022  - Kilimanoor 0470-2672217  - Kodungaloor 0480-2803155  - Kollam 0474-2752008  - Konni 0468-2244555 ( - Koothattukulam 0468-2253444  - Kothamangalam 0485-2862202 - Kottarakkara 0474-2452622  - Kottayam 0481-2562908  - Kozhikode 0495-2723796  - Kulathupuzha 0475-2318777  - Kumaly 0486-9224242  - Mala 0480-2890438  - Malappuram 0483-2734950 - Mallapally 0469-2785080  - Mananthavady 0493-5240640  - Mannarghat 0492-4225150  - Mavelikara 0479-2302282  - Moolamattom 0486-2252045  - Moovattupuzha 0485-2832321 - Munnar 0486-5230201  - Nedumangad 0472-2812235  - Nedumkandam 04868-234533  - Neyyatinkara 0471-2222243  - Nilambur 04931-223929  - North Paravur 0484-2442373  - Pala 0482-2212250  - Palakkad 0491-2520098  - Palode 0472-2840259  - Pamba 0473-5203445  - Pandalam 0473-4255800 - Pappanamcode 0471-2494002  - Parassala 0471-2202058  - Pathanamthitta 0468-2222366  - Pathanapuram 0475-2354010 - Payyanur 0498-5203062  - Perinthamana 0493-3227342  - Peroorkada 0471-2433683  - Perumbavoor 0484-2523416  - Piravom 0485-2265533 (24 X 7) - Ponkunnam 0482-8221333 - Ponnani 0494-2666396 - Poovar 0471-2210047  - Punalur 0475-2222626 - Puthukkadu 0480-2751648  - Ranni 04735-225253 - Sulthan Bathery 0493-6220217  - Thalassery 0490-2343333  - Thamarassery 0495-2222217  - Thiruvalla 0469-2602945  - Thiruvambady 0495-2254500  - Thodupuzha 0486-2222388  - Thottilpalam 0496-2566200  - Thrissur 0487-2421150 - Tvm Central 0471-2323886  - Tvm City 0471-2575495  - Vadakara 0496-2523377  - Vadakkanchery 0492-2255001  - Vaikom 0482-9231210  - Vellanad 0472-2884686  - Venjaramoodu 0472-2874141  - Vikasbhavan 0471-2307890 - Vithura 0472-2858686  - Vizhinjam 0471-2481365 - Vellarada 0471 2242029</p>"},{"location":"blog/page/2/","title":"Hello","text":""}]}